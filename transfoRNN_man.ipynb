{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to make take a transformer and make it infinite depth and remove redundancy with as few changes as possible.\n",
    "# change 1 - do universal transformer style - let depth grow until quit\n",
    "# change 2 - to avoid redendancy in universal Transfo computations, start each token with the final hidden state from the previous layer - the removes token parallelism\n",
    "# change 3 - it used to be that all MLPs at depth N time T can attend to all MLPs at depth<N, time<=T, so long term info is left uninterrupted. With this change, since all tokens are now vertically stacked, attention can only attend to the 1 token that calls it - can't long termattend to previos. TO fix this, the naive solution is to do attention over the whole vertical stack depth<N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we cut a token's depth whenever the NN meets tha threshold, there's no backprop for what if it let it go sooner. \n",
    "# So, we need to provide signal for what if we cut it off earlier or later. earlier is essentially done by a per-token depth penalty - the NN knows if it triggered sooner, it would get lower loss\n",
    "\n",
    "# issue:\n",
    "# if we cut it off, it doesnt know it could have gotten better.\n",
    "# We should cut it off AFTER it says to cut off - this way it can identify if it should have actually triggerd later. \n",
    "# universal transformer and CALM must also face this problem - what does their code look like?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
